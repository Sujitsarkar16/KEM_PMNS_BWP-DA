"""
Comprehensive Comparison Report
Compares all models: CatBoost Baseline, Stacking Ensemble, and previous baselines
"""

import pandas as pd
import json
from pathlib import Path
from datetime import datetime

# Paths
BASE_DIR = Path(r"e:\KEM\Project")
RESULTS_DIR = BASE_DIR / "Advanced_Stacking_Models" / "Results"
OUTPUT_DIR = RESULTS_DIR

def load_latest_results():
    """Load the latest results from each model type"""
    results = {}
    
    # CatBoost Baseline
    catboost_files = sorted(RESULTS_DIR.glob("catboost_baseline_metrics_*.json"))
    if catboost_files:
        with open(catboost_files[-1], 'r') as f:
            results['CatBoost_Baseline'] = json.load(f)
    
    # Stacking Ensemble
    stacking_files = sorted(RESULTS_DIR.glob("stacking_ensemble_metrics_*.json"))
    if stacking_files:
        with open(stacking_files[-1], 'r') as f:
            results['Stacking_Ensemble'] = json.load(f)
    
    return results

def create_comparison_table(results):
    """Create comprehensive comparison table"""
    comparison_data = []
    
    for model_name, data in results.items():
        row = {
            'Model': model_name,
            'CV_RMSE_Mean': data['cv_metrics']['rmse_mean'],
            'CV_RMSE_Std': data['cv_metrics']['rmse_std'],
            'CV_R2_Mean': data['cv_metrics']['r2_mean'],
            'CV_R2_Std': data['cv_metrics']['r2_std'],
            'CV_MAE_Mean': data['cv_metrics']['mae_mean'],
            'CV_MAE_Std': data['cv_metrics'].get('mae_std', None),  # Handle optional field
            'Full_RMSE': data['full_metrics']['rmse'],
            'Full_R2': data['full_metrics']['r2'],
            'Full_MAE': data['full_metrics']['mae']
        }
        comparison_data.append(row)

    
    df = pd.DataFrame(comparison_data)
    
    # Sort by CV RMSE (best first)
    df = df.sort_values('CV_RMSE_Mean')
    
    return df

def generate_markdown_report(comparison_df, results):
    """Generate markdown report"""
    report = []
    report.append("# üî¨ Model Comparison Report - Advanced Stacking Models")
    report.append(f"\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    report.append("## üìä Performance Comparison")
    report.append("")
    report.append("### Cross-Validation Metrics (5-Fold)")
    report.append("")
    report.append("| Model | RMSE (Mean ¬± Std) | R¬≤ (Mean ¬± Std) | MAE (Mean ¬± Std) |")
    report.append("|-------|-------------------|-----------------|------------------|")
    
    for _, row in comparison_df.iterrows():
        rmse_str = f"{row['CV_RMSE_Mean']:.4f} ¬± {row['CV_RMSE_Std']:.4f}"
        r2_str = f"{row['CV_R2_Mean']:.4f} ¬± {row['CV_R2_Std']:.4f}"
        # Handle cases where MAE_Std might be missing
        if 'CV_MAE_Std' in row and pd.notna(row.get('CV_MAE_Std')):
            mae_str = f"{row['CV_MAE_Mean']:.4f} ¬± {row['CV_MAE_Std']:.4f}"
        else:
            mae_str = f"{row['CV_MAE_Mean']:.4f}"
        report.append(f"| {row['Model']} | {rmse_str} | {r2_str} | {mae_str} |")

    
    report.append("")
    report.append("### Full Dataset Metrics")
    report.append("")
    report.append("| Model | RMSE | R¬≤ | MAE |")
    report.append("|-------|------|----|----|")
    
    for _, row in comparison_df.iterrows():
        report.append(f"| {row['Model']} | {row['Full_RMSE']:.4f} | {row['Full_R2']:.4f} | {row['Full_MAE']:.4f} |")
    
    # Best model
    best_model = comparison_df.iloc[0]
    report.append("")
    report.append("## üèÜ Best Model")
    report.append("")
    report.append(f"**{best_model['Model']}**")
    report.append(f"- CV RMSE: {best_model['CV_RMSE_Mean']:.4f} ¬± {best_model['CV_RMSE_Std']:.4f}")
    report.append(f"- CV R¬≤: {best_model['CV_R2_Mean']:.4f}")
    report.append(f"- Full RMSE: {best_model['Full_RMSE']:.4f}")
    report.append("")
    
    # Model details
    report.append("## üìù Model Details")
    report.append("")
    
    for model_name, data in results.items():
        report.append(f"### {model_name}")
        report.append("")
        
        if 'base_models' in data:
            report.append(f"- **Architecture**: Stacking Ensemble")
            report.append(f"- **Base Models**: {', '.join(data['base_models'])}")
            report.append(f"- **Meta-Learner**: {data['meta_learner']}")
        elif 'parameters' in data:
            report.append(f"- **Model Type**: CatBoost Regressor")
            report.append(f"- **Parameters**:")
            for key, value in data['parameters'].items():
                report.append(f"  - {key}: {value}")
        
        report.append("")
    
    # Power features
    report.append("## üß¨ Power Interaction Features")
    report.append("")
    report.append("1. **Genetic_Volume** = `f0_m_ht √ó f0_f_head_cir_ini`")
    report.append("   - Theory: Birth weight constrained by maternal height (uterine capacity)")
    report.append("   - but driven by paternal genetics (skeletal potential)")
    report.append("")
    report.append("2. **Placental_Efficiency_Proxy** = `f0_m_plac_wt / f0_m_wt_prepreg`")
    report.append("   - Theory: Biological efficiency score of reproductive system")
    report.append("")
    
    # Recommendations
    report.append("## üí° Key Insights")
    report.append("")
    report.append("1. **Stacking > Tuning**: Combining complementary models (XGBoost + CatBoost + Linear)")
    report.append("   provides better performance than extensive hyperparameter tuning of a single model.")
    report.append("")
    report.append("2. **CatBoost Advantage**: Ordered boosting handles small datasets (n=793) better")
    report.append("   than traditional gradient boosting, reducing overfitting.")
    report.append("")
    report.append("3. **Biological Features**: Power interaction features capture intergenerational")
    report.append("   genetic and physiological effects that are crucial for birth weight prediction.")
    report.append("")
    
    return "\n".join(report)

def main():
    print("=" * 80)
    print("GENERATING COMPARISON REPORT")
    print("=" * 80)
    print()
    
    # Load results
    print("Loading Results...")
    print("-" * 80)
    results = load_latest_results()
    
    if not results:
        print("‚ùå No results found! Please run the models first:")
        print("   1. python feature_engineering.py")
        print("   2. python catboost_baseline.py")
        print("   3. python stacking_ensemble.py")
        return
    
    print(f"‚úì Loaded results for {len(results)} models:")
    for model_name in results.keys():
        print(f"  ‚Ä¢ {model_name}")
    print()
    
    # Create comparison table
    print("Creating Comparison Table...")
    print("-" * 80)
    comparison_df = create_comparison_table(results)
    print(comparison_df.to_string(index=False))
    print()
    
    # Save CSV
    csv_path = OUTPUT_DIR / "model_comparison_summary.csv"
    comparison_df.to_csv(csv_path, index=False)
    print(f"‚úì Saved comparison table to: {csv_path}")
    print()
    
    # Generate markdown report
    print("Generating Markdown Report...")
    print("-" * 80)
    report = generate_markdown_report(comparison_df, results)
    
    report_path = OUTPUT_DIR / "COMPARISON_REPORT.md"
    with open(report_path, 'w') as f:
        f.write(report)
    print(f"‚úì Saved report to: {report_path}")
    print()
    
    # Summary
    best_model = comparison_df.iloc[0]
    print("=" * 80)
    print("REPORT GENERATION COMPLETE")
    print("=" * 80)
    print(f"\nüèÜ Best Model: {best_model['Model']}")
    print(f"  ‚Ä¢ CV RMSE: {best_model['CV_RMSE_Mean']:.4f} ¬± {best_model['CV_RMSE_Std']:.4f}")
    print(f"  ‚Ä¢ CV R¬≤:   {best_model['CV_R2_Mean']:.4f}")
    print()
    print(f"üìÑ Full report: {report_path}")
    print()

if __name__ == "__main__":
    main()
